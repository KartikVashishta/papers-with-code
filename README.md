# Deep Learning Paper Implementations

This repository serves as a personal learning journey through important papers in deep learning, starting with foundational architectures and gradually expanding to more complex models. Each implementation is meant to be a clean, educational reference point with a focus on understanding the core concepts.

## Current Implementations

| Paper                                                         | Implementation                                             | Key Concepts                                                                                                            |
| ------------------------------------------------------------- | ---------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------- |
| [Attention Is All You Need](https://arxiv.org/abs/1706.03762) | [transformer-implementation/](transformer-implementation/) | - Multi-Head Attention<br>- Positional Encoding<br>- Layer Normalization<br>- Label Smoothing<br>- Warmup Learning Rate |

## Transformer Implementation Details

The current implementation includes a complete transformer architecture with:

- Multi-headed self-attention mechanism
- Position-wise feed-forward networks
- Positional encodings
- Layer normalization
- Encoder and decoder stacks
- Label smoothing
- Learning rate scheduling with warmup

## Note

These implementations are meant for educational purposes and self-reference. While they aim to be correct, they may not be optimized for production use. They serve as a starting point for understanding the underlying concepts and architectures described in the papers.
